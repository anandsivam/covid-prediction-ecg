{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c624664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "\n",
    "# import imutils\n",
    "# import itertools\n",
    "# import shutil\n",
    "# import random\n",
    "# import glob\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02026858",
   "metadata": {},
   "source": [
    "### Processing Covid19 Patients ECG images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8d084ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gammaCorrection(src, gamma):\n",
    "    invGamma = 1 / gamma\n",
    " \n",
    "    table = [((i / 255) ** invGamma) * 255 for i in range(256)]\n",
    "    table = np.array(table, np.uint8)\n",
    " \n",
    "    return cv2.LUT(src, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa32179",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_path = \"C:\\\\Users\\\\anant\\\\jupyterBooks\\\\LectureNotebooks\\\\2022W\\\\AML_2404_LAB\\\\data1\"\n",
    "\n",
    "for foldername in os.listdir(main_path): # looping over the different class folder\n",
    "    \n",
    "    folder = main_path+\"/\"+foldername+\"/\"\n",
    "    Copy_to_path = folder+\"/processed/\"\n",
    "\n",
    "    for index, filename in enumerate(os.listdir(folder)): # retrieving the image files in particular folder\n",
    "        \n",
    "        if filename != 'processed':\n",
    "            \n",
    "            image = cv2.imread(folder+filename, cv2.IMREAD_GRAYSCALE) # grayscale\n",
    "\n",
    "            enhance = cv2.equalizeHist(image) # equalize histogram used to enchance the lines\n",
    "\n",
    "            thrsh = cv2.adaptiveThreshold(enhance,255,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY,41,21) \n",
    "\n",
    "            contrs, _ = cv2.findContours(thrsh.copy(),cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE) # using threshold to get the contours\n",
    "            contrs = list(contrs)\n",
    "\n",
    "            c = max(contrs, key = cv2.contourArea) # getting the largest contours that is the largest shape ( image boundary )\n",
    "\n",
    "            print(c.shape)\n",
    "            if c.shape[0] <= 4:            \n",
    "\n",
    "                contrs.remove(max(contrs, key = cv2.contourArea)) # removing the frame\n",
    "\n",
    "                c = max(contrs, key = cv2.contourArea) # again getting the largest frame that is the pulse frame\n",
    "\n",
    "                x,y,w,h = cv2.boundingRect(c) # getting the co-ordinates\n",
    "\n",
    "\n",
    "                rect = image[y:y+h, x:x+w] # cropping the frame\n",
    "\n",
    "                cv2.rectangle(image,(x,y),(x+w,y+h),(0,255,0),10) # drawing rectangle with the co-ordinates\n",
    "\n",
    "                gamma_corrected = gammaCorrection(rect,3) # gamma correction\n",
    "\n",
    "                (thresh, im_bw) = cv2.threshold(gamma_corrected, 50, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU) # threshold is increased\n",
    "\n",
    "                im_bw_resize = cv2.resize(im_bw,(720, 576)) # resizing the image to 256\n",
    "                img = Image.fromarray(im_bw_resize)\n",
    "                print(filename, \" \", index)\n",
    "                img.save(Copy_to_path+f'{foldername}_{index}.jpg', 'JPEG')\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45595ff",
   "metadata": {},
   "source": [
    "### Data Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1043930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02e7a7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anant\\jupyterBooks\\LectureNotebooks\\2022W\\AML_2404_LAB\\data1\\augmented\\abnormalHR\\\n",
      "538\n",
      "batch size for abnormalHR category is 4\n",
      "C:\\Users\\anant\\jupyterBooks\\LectureNotebooks\\2022W\\AML_2404_LAB\\data1\\augmented\\covid19\\\n",
      "250\n",
      "batch size for covid19 category is 8\n",
      "C:\\Users\\anant\\jupyterBooks\\LectureNotebooks\\2022W\\AML_2404_LAB\\data1\\augmented\\historyMI\\\n",
      "199\n",
      "batch size for historyMI category is 8\n",
      "C:\\Users\\anant\\jupyterBooks\\LectureNotebooks\\2022W\\AML_2404_LAB\\data1\\augmented\\miPatients\\\n",
      "72\n",
      "batch size for miPatients category is 8\n",
      "C:\\Users\\anant\\jupyterBooks\\LectureNotebooks\\2022W\\AML_2404_LAB\\data1\\augmented\\normal\\\n",
      "854\n",
      "batch size for normal category is 2\n"
     ]
    }
   ],
   "source": [
    "main_path = \"C:\\\\Users\\\\anant\\\\jupyterBooks\\\\LectureNotebooks\\\\2022W\\\\AML_2404_LAB\\\\data1\"\n",
    "augmented_path = \"C:\\\\Users\\\\anant\\\\jupyterBooks\\\\LectureNotebooks\\\\2022W\\\\AML_2404_LAB\\\\data1\\\\augmented\"\n",
    "\n",
    "data_gen_args = dict(\n",
    "                     width_shift_range=0.1,\n",
    "                     height_shift_range=0.1,\n",
    "                     zoom_range=0.2,\n",
    "                     fill_mode='constant',\n",
    "                     cval=255\n",
    "                    )\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "for foldername in os.listdir(main_path): # looping over the different class folder\n",
    "    if foldername != 'augmented':\n",
    "        \n",
    "        folder = main_path+\"\\\\\"+foldername+\"\\\\processed\\\\\"\n",
    "\n",
    "        Copy_to_path = augmented_path+'\\\\'+foldername+'\\\\'\n",
    "        print(Copy_to_path)\n",
    "\n",
    "    #     determining the files count in each category\n",
    "    #     we want 7500 images in final dataset\n",
    "    #     we have five categories\n",
    "    #     so we need 1500 images in each category\n",
    "        files_count = os.listdir(folder) \n",
    "        print(len(files_count))\n",
    "\n",
    "        total_image_present = len(files_count)\n",
    "        img_batch_size = round(2000/total_image_present)\n",
    "        if img_batch_size > 8 :\n",
    "            img_batch_size = 8\n",
    "\n",
    "        print(f\"batch size for {foldername} category is {img_batch_size}\")\n",
    "        file_count = 0\n",
    "        for filename in os.listdir(folder): # retrieving the image files in particular folder\n",
    "\n",
    "            img = Image.open(folder+filename)\n",
    "            \n",
    "            file_count += 1\n",
    "            #PIL images into NumPy arrays\n",
    "            array = img_to_array(img)\n",
    "            reshaped_np_arr = array.reshape((1,)+array.shape)\n",
    "            count = 0\n",
    "\n",
    "    #             generating the augmentation of images\n",
    "            for batch in image_datagen.flow(\n",
    "                reshaped_np_arr,\n",
    "                batch_size=1,\n",
    "                save_to_dir=Copy_to_path,\n",
    "                save_prefix = foldername+f'{file_count}',\n",
    "                save_format='jpeg',\n",
    "                seed=20\n",
    "            ):\n",
    "\n",
    "                count += 1\n",
    "                if count == img_batch_size:\n",
    "                    break\n",
    "                    \n",
    "                            \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfe0655",
   "metadata": {},
   "source": [
    "### Test Train Split of all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7272e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: split-folders in c:\\programdata\\anaconda3\\lib\\site-packages (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install split-folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5359086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c44ebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_folder = \"C:\\\\Users\\\\anant\\\\jupyterBooks\\\\LectureNotebooks\\\\2022W\\\\AML_2404_LAB\\\\data1\\\\augmented\"\n",
    "output_folder = \"C:\\\\Users\\\\anant\\\\jupyterBooks\\\\LectureNotebooks\\\\2022W\\\\AML_2404_LAB\\\\data1\\\\inputImages\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7a20bd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 8032 files [00:05, 1482.88 files/s]\n"
     ]
    }
   ],
   "source": [
    "splitfolders.ratio(augmented_folder, output=output_folder,\n",
    "    seed=28, ratio=(.75, .25), group_prefix=None, move=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848babe1",
   "metadata": {},
   "source": [
    "# Reading the Test and Val images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f8e1c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abf32d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5fbcac",
   "metadata": {},
   "source": [
    "##### Reading Training Data using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46b0658b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6024 files belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_folder = \"C:\\\\Users\\\\anant\\\\jupyterBooks\\\\LectureNotebooks\\\\2022W\\\\AML_2404_LAB\\\\data1\\\\inputImages\\\\train\"\n",
    "\n",
    "training_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_folder, \n",
    "    labels='inferred', \n",
    "    label_mode='int',\n",
    "    color_mode='grayscale',\n",
    "    batch_size=64,\n",
    "    image_size=(720, 576),\n",
    "    shuffle=False, seed=11,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    follow_links=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cb431f",
   "metadata": {},
   "source": [
    "##### Reading Test/val Data using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fbdbca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2008 files belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "val_folder = \"C:\\\\Users\\\\anant\\\\jupyterBooks\\\\LectureNotebooks\\\\2022W\\\\AML_2404_LAB\\\\data1\\\\inputImages\\\\val\"\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    val_folder, \n",
    "    labels='inferred', \n",
    "    label_mode='int',\n",
    "    color_mode='grayscale',\n",
    "    batch_size=64,\n",
    "    image_size=(720, 576),\n",
    "    shuffle=False, seed=11,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    follow_links=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4146a22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.BatchDataset"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34d664c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for image in training_ds.take(1):\n",
    "    image\n",
    "    count += 1\n",
    "    if count == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ef126",
   "metadata": {},
   "source": [
    "# Model Creation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8395e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "314544b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_rows, img_cols = 720, 576\n",
    "# input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cbcb36",
   "metadata": {},
   "source": [
    "###  CNN with 3 Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed845282",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn3 = Sequential()\n",
    "cnn3.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(720,576,1)))\n",
    "cnn3.add(BatchNormalization())\n",
    "\n",
    "cnn3.add(MaxPooling2D((2, 2)))\n",
    "cnn3.add(Dropout(0.25))\n",
    "\n",
    "cnn3.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "cnn3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn3.add(Dropout(0.25))\n",
    "\n",
    "cnn3.add(Conv2D(128, kernel_size=(2, 2), activation='relu'))\n",
    "cnn3.add(Dropout(0.4))\n",
    "\n",
    "cnn3.add(Flatten())\n",
    "\n",
    "cnn3.add(Dense(128, activation='relu'))\n",
    "cnn3.add(Dropout(0.3))\n",
    "cnn3.add(Dense(5, activation='softmax'))\n",
    "\n",
    "cnn3.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043b9c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      " 7/95 [=>............................] - ETA: 1:38:23 - loss: 0.6113 - accuracy: 0.8638"
     ]
    }
   ],
   "source": [
    "cnn3.fit(training_ds,\n",
    "         batch_size=64,\n",
    "         epochs=11,\n",
    "         verbose='auto',\n",
    "         use_multiprocessing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7c8ab1",
   "metadata": {},
   "source": [
    "### CNN with 4 Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b68a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn4 = Sequential()\n",
    "cnn4.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "cnn4.add(BatchNormalization())\n",
    "\n",
    "cnn4.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "cnn4.add(BatchNormalization())\n",
    "cnn4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn4.add(Dropout(0.25))\n",
    "\n",
    "cnn4.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "cnn4.add(BatchNormalization())\n",
    "cnn4.add(Dropout(0.25))\n",
    "\n",
    "cnn4.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "cnn4.add(BatchNormalization())\n",
    "cnn4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn4.add(Dropout(0.25))\n",
    "\n",
    "cnn4.add(Flatten())\n",
    "\n",
    "cnn4.add(Dense(512, activation='relu'))\n",
    "cnn4.add(BatchNormalization())\n",
    "cnn4.add(Dropout(0.5))\n",
    "\n",
    "cnn4.add(Dense(128, activation='relu'))\n",
    "cnn4.add(BatchNormalization())\n",
    "cnn4.add(Dropout(0.5))\n",
    "\n",
    "cnn4.add(Dense(5, activation='softmax'))\n",
    "\n",
    "cnn4.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb23d9f",
   "metadata": {},
   "source": [
    "# Testing and Junkies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39b4a18",
   "metadata": {},
   "source": [
    "### Do Not Run These unless testing something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8e8767c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anant\\jupyterBooks\\LectureNotebooks\\2022W\\AML_2404_LAB\\data1\\augmented\\abnormalHR\\cropped\\\n",
      "C:\\Users\\anant\\jupyterBooks\\LectureNotebooks\\2022W\\AML_2404_LAB\\data1\\augmented\\covid19\\cropped\\\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24084/1957532539.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'cropped'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\\\\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#reading the images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[0mimgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#converting the image to grayscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "augmented_path = \"C:\\\\Users\\\\anant\\\\jupyterBooks\\\\LectureNotebooks\\\\2022W\\\\AML_2404_LAB\\\\data1\\\\augmented\"\n",
    "        \n",
    "\n",
    "for foldername in os.listdir(augmented_path): # retrieving the image files in the particular folder\n",
    "    \n",
    "    folder = augmented_path+\"\\\\\"+foldername\n",
    "    copy_path = augmented_path+'\\\\'+foldername+'\\\\cropped\\\\'\n",
    "    print(copy_path)\n",
    "    count = 0\n",
    "    \n",
    "    for filename in os.listdir(folder):\n",
    "    \n",
    "        if filename != 'cropped':\n",
    "            count += 1\n",
    "            image = cv2.imread(folder+\"\\\\\"+filename) #reading the images\n",
    "            imgray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #converting the image to grayscale\n",
    "            \n",
    "            ret, thresh = cv2.threshold(imgray, 0, 1, 0) #finding image threshold\n",
    "            contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "            # finding all the contours using chain approx none, chain_approx_simple creating many no of contours\n",
    "            \n",
    "            contour_max = max(contours, key = cv2.contourArea) # finding the highest contour for cropping the image\n",
    "            \n",
    "            x,y,w,h = cv2.boundingRect(contour_max) # getting the co-ordinates\n",
    "            image = image[y:y+h, x:x+w] # cropping the frame\n",
    "            cv2.imwrite(copy_path + f'{foldername}_{count}.jpg', image)\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b458b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d49a82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbba8a1",
   "metadata": {},
   "source": [
    "## Testing Augmenting image code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50f65a94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abnormalHR_0.jpg_0_840.jpeg', 'abnormalHR_0_cropped.jpeg.jpg', 'abnormalHR_124_cropped.jpeg.jpg', 'abnormalHR_14.jpg', 'abnormalHR_24.jpg_0_9181.jpeg', 'Screenshot 2022-03-28 222642.jpg']\n"
     ]
    }
   ],
   "source": [
    "path = \"C:\\\\Users\\\\anant\\\\jupyterBooks\\\\LectureNotebooks\\\\2022W\\\\AML_2404_LAB\\\\data\\\\test\"\n",
    "os.chdir(path)\n",
    "print(os.listdir(os.curdir))\n",
    "filename = \"abnormalHR_14\"\n",
    "\n",
    "img_batch_size = 8\n",
    "\n",
    "data_gen_args = dict(\n",
    "                     width_shift_range=0.1,\n",
    "                     height_shift_range=0.1,\n",
    "                     zoom_range=0.2,\n",
    "                     fill_mode='constant',\n",
    "                     cval=255\n",
    "                    )\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "\n",
    "img = cv2.imread(\"abnormalHR_14.jpg\")\n",
    "\n",
    "\n",
    "reshaped_np_arr = img.reshape((1,)+img.shape)\n",
    "count = 0\n",
    "\n",
    "#             generating the augmentation of images\n",
    "for batch in image_datagen.flow(\n",
    "    reshaped_np_arr,\n",
    "    batch_size=1,\n",
    "    save_to_dir=os.curdir,\n",
    "    save_prefix = filename,\n",
    "    save_format='jpeg',\n",
    "    seed=20\n",
    "):\n",
    "\n",
    "    count += 1\n",
    "    if count == img_batch_size:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d94496cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abnormalHR_0.jpg_0_840.jpeg']\n",
      "<class 'tuple'>\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "path = \"C:\\\\Users\\\\anant\\\\jupyterBooks\\\\LectureNotebooks\\\\2022W\\\\AML_2404_LAB\\\\data\\\\test\"\n",
    "os.chdir(path)\n",
    "print(os.listdir(os.curdir))\n",
    "img = cv2.imread('abnormalHR_0.jpg_0_840.jpeg')\n",
    "\n",
    "imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "ret, thresh = cv2.threshold(imgray, 0, 1, 0)\n",
    "contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "print(type(contours))\n",
    "print(len(contours))\n",
    "\n",
    "if len(contours) != 0:\n",
    "        \n",
    "    c = max(contours, key = cv2.contourArea)\n",
    "    contour_image = cv2.drawContours(img, contours, -1, (255,0,0), 3)\n",
    "    cv2.imshow(\"out\", contour_image)\n",
    "    cv2.waitKey(0)\n",
    "    x,y,w,h = cv2.boundingRect(c)\n",
    "        \n",
    "    rect = contour_image[y:y+h, x:x+w]\n",
    "    cv2.imshow(\"out\", rect)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5467eb50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abnormalHR_0.jpg_0_840.jpeg', 'abnormalHR_0_cropped.jpeg.jpg', 'abnormalHR_124_cropped.jpeg.jpg', 'abnormalHR_24.jpg_0_9181.jpeg', 'Screenshot 2022-03-28 222642.jpg']\n",
      "[[[ -1  -1   1  -1]\n",
      "  [  2  -1  -1   0]\n",
      "  [  3   1  -1   0]\n",
      "  ...\n",
      "  [358 356  -1   0]\n",
      "  [359 357  -1   0]\n",
      "  [ -1 358  -1   0]]]\n",
      "360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"C:\\\\Users\\\\anant\\\\jupyterBooks\\\\LectureNotebooks\\\\2022W\\\\AML_2404_LAB\\\\data\\\\test\"\n",
    "os.chdir(path)\n",
    "print(os.listdir(os.curdir))\n",
    "filename = \"abnormalHR_124_cropped.jpeg\"\n",
    "image = cv2.imread(\"abnormalHR_24.jpg_0_9181.jpeg\") #reading the images\n",
    "\n",
    "imgray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #converting the image to grayscale\n",
    "ret, thresh = cv2.threshold(imgray, 0, 1, 0) #finding image threshold\n",
    "contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# finding all the contours using chain approx none, chain_approx_simple creating many no of contours\n",
    "print(hierarchy)\n",
    "print(len(contours))\n",
    "\n",
    "contour_max = max(contours, key = cv2.contourArea) # finding the highest contour for cropping the image\n",
    "\n",
    "x,y,w,h = cv2.boundingRect(contour_max) # getting the co-ordinates\n",
    "\n",
    "image = image[y:y+h, x:x+w] # cropping the frame\n",
    "cv2.imwrite(f'{filename}.jpg', image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "650ee01d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abnormalHR_0.jpg_0_840.jpeg']\n",
      "<class 'numpy.ndarray'>\n",
      "(101, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "path = \"C:\\\\Users\\\\anant\\\\jupyterBooks\\\\LectureNotebooks\\\\2022W\\\\AML_2404_LAB\\\\data\\\\test\"\n",
    "os.chdir(path)\n",
    "print(os.listdir(os.curdir))\n",
    "img = cv2.imread('abnormalHR_0.jpg_0_840.jpeg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# contours, _ = cv2.findContours(...) # Your call to find the contours using OpenCV 2.4.x\n",
    "blur = cv2.GaussianBlur(img,(5,5),0)\n",
    "ret, threshold = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "contrs, _ = cv2.findContours(threshold,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE) # using threshold to get the contours\n",
    "\n",
    "contrs_list = list(contrs)\n",
    "contrs_list.remove(max(contrs_list, key = cv2.contourArea)) # removing the frame\n",
    "# again getting the largest frame that is the pulse frame\n",
    "\n",
    "c = max(contrs_list, key = cv2.contourArea)\n",
    "\n",
    "\n",
    "x,y,w,h = cv2.boundingRect(c) # getting the co-ordinates\n",
    "\n",
    "\n",
    "rect = img[y:y+h, x:x+w] # cropping the frame\n",
    "\n",
    "#                 cv2.rectangle(image,(x,y),(x+w,y+h),(0,255,0),10) # drawing rectangle with the co-ordinates\n",
    "\n",
    "#                 gamma_corrected = gammaCorrection(rect,3) # gamma correction\n",
    "\n",
    "#                 (thresh, im_bw) = cv2.threshold(gamma_corrected, 50, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU) # threshold is increased\n",
    "\n",
    "#                 im_bw_resize = cv2.resize(im_bw,(720, 576)) # resizing the image to 256\n",
    "# img = Image.fromarray(rect)\n",
    "\n",
    "\n",
    "cv2.imshow('image',rect)\n",
    "# cv2.drawContours(img, contrs, 3, (0,255,0), 3)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "#thrsh = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY,41,21) \n",
    "\n",
    "# canny_output = cv2.Canny(img, thrsh, thrsh * 2)\n",
    "# contours, _ = cv2.findContours(thrsh.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) # Your call to find the contours\n",
    "# print(contours)\n",
    "\n",
    "# # Draw contours\n",
    "# drawing = np.zeros((canny_output.shape[0], canny_output.shape[1], 3), dtype=np.uint8)\n",
    "# for i in range(len(contours)):\n",
    "#     color = (rng.randint(0,256), rng.randint(0,256), rng.randint(0,256))\n",
    "#     cv.drawContours(drawing, contours, i, color, 2, cv.LINE_8, hierarchy, 0)\n",
    "# # Show in a window\n",
    "# cv.imshow('Contours', drawing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185707d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2 # The index of the contour that surrounds your object\n",
    "mask = np.zeros_like(img) # Create mask where white is what we want, black otherwise\n",
    "cv2.drawContours(mask, contours, idx, 255, -1) # Draw filled contour in mask\n",
    "out = np.zeros_like(img) # Extract out the object and place into output image\n",
    "out[mask == 255] = img[mask == 255]\n",
    "\n",
    "# Show the output image\n",
    "cv2.imshow('Output', out)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f54791c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abnormalHR_0.jpg_0_840.jpeg']\n"
     ]
    }
   ],
   "source": [
    "path = \"C:\\\\Users\\\\anant\\\\jupyterBooks\\\\LectureNotebooks\\\\2022W\\\\AML_2404_LAB\\\\data\\\\test\"\n",
    "os.chdir(path)\n",
    "print(os.listdir(os.curdir))\n",
    "img = cv2.imread('abnormalHR_0.jpg_0_840.jpeg') # Read in your image\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "ret, mask = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU) \n",
    "\n",
    "kernel = np.ones((9,9), np.uint8)\n",
    "mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "# put mask into alpha channel of result\n",
    "result = img.copy()\n",
    "result = cv2.cvtColor(result, cv2.COLOR_BGR2BGRA)\n",
    "result[:, :, 3] = mask\n",
    "\n",
    "# save resulting masked image\n",
    "#cv2.imwrite('retina_masked.png', result)\n",
    "cv2.imshow('Output', result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed620adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = \"data/covid19/\"\n",
    "\n",
    "\n",
    "# for filename in os.listdir(PATH):\n",
    "#     img = Image.open(os.path.join(PATH, filename)) # images are color images\n",
    "#     img = img.resize((1920,1080), Image.BICUBIC)\n",
    "#     img.save(Copy_to_path+filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a0cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# filename = 'data/covid19/Binder1_Page_001.jpg'\n",
    "\n",
    "# size = (720, 576)\n",
    "# count = 0\n",
    "# for filename in g\n",
    "# with Image.open(filename) as im:\n",
    "#     gray_img = ImageOps.grayscale(im)\n",
    "#     gray_img.save('data/covid19/processed/covid001'+'.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b2cf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"data/covid19/Binder1_Page_017\"\n",
    "# Copy_to_path=\"data/covid19/processed/\"\n",
    "\n",
    "# count = 0\n",
    "\n",
    "# for filename in glob.glob(f'{file_path}.jpg'):\n",
    "#     count += 1\n",
    "#     with Image.open(filename) as img:\n",
    "#         img = img.convert('L')\n",
    "#         img = img.filter(ImageFilter.SHARPEN)\n",
    "#         img = img.filter(ImageFilter.EDGE_ENHANCE)\n",
    "        \n",
    "#         #img = img.filter(ImageFilter.CONTOUR)    \n",
    "#         #img = img.filter(ImageFilter.EMBOSS)\n",
    "#         #img = img.filter(ImageFilter.EDGE_ENHANCE)\n",
    "#         image = np.array(img) # converting PIL image to numpy array\n",
    "        \n",
    "#         thrsh = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 41, 21) \n",
    "\n",
    "#         contrs, _ = cv2.findContours(thrsh.copy(),cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE) # using threshold to get the contours\n",
    "#         contrs = list(contrs)\n",
    "        \n",
    "#         c = max(contrs, key = cv2.contourArea)\n",
    "        \n",
    "#         contrs.remove(max(contrs, key = cv2.contourArea)) # removing the frame\n",
    "\n",
    "#         c = max(contrs, key = cv2.contourArea) # again getting the largest frame that is the pulse frame\n",
    "\n",
    "#         x,y,w,h = cv2.boundingRect(c) # getting the co-ordinates\n",
    "\n",
    "\n",
    "#         rect = image[y:y+h, x:x+w] # cropping the frame\n",
    "\n",
    "#         cv2.rectangle(image,(x,y),(x+w,y+h),(0,255,0),10) # drawing rectangle with the co-ordinates\n",
    "\n",
    "#         gamma_corrected = gammaCorrection(rect,3) # gamma correction\n",
    "\n",
    "#         (thresh, im_bw) = cv2.threshold(gamma_corrected, 50, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU) # threshold is increased\n",
    "        \n",
    "#         print(type(im_bw))\n",
    "#         processed_img = Image.fromarray(im_bw) #Converting the array to image\n",
    "#         processed_img.show()\n",
    "#         processed_img.save(Copy_to_path+f'/covid19_{count}.jpg', 'JPEG') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e882a52b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
